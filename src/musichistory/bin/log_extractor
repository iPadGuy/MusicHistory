#!/home/patrick/.local/share/virtualenvs/MusicHistory-nP00uPHO/bin/python3
# -*- coding: utf-8 -*-
# log_extractor3 - Tuesday, March 18, 2025
""" Extract & analyze timestamps and filenames from collected SMPlayer logs (via log_collector)
	and generate Top XXX lists """
__version__ = "0.7.40"

import builtins
import concurrent.futures
import lzma
import os
import re
import shutil
import sys
import tarfile
from contextlib import contextmanager
from datetime import datetime, timedelta, timezone
from environs import Env
from glob import glob
from os.path import basename, dirname, getmtime, getsize, exists, join
from pathlib import Path
from subprocess import run, CalledProcessError
from time import sleep
from timeit import default_timer as timer

import click
import pandas as pd
import sqlalchemy as sa
import xdg_base_dirs as xdg
from dateutil.parser import parse, ParserError
from loguru import logger
from sqlalchemy import create_engine, text
# from tabulate import tabulate
# from xdg import XDG_DATA_HOME, XDG_CONFIG_HOME

appname = "MusicHistory"
_basedir = Path(__file__).resolve().parent.parent
__org__ = "LevellTech"
__project__ = _basedir.stem
# __project__ = appname
__module__ = Path(__file__).resolve().stem
# config_dir = xdg.xdg_config_home()
_config_dir = xdg.xdg_config_home() / __org__ / __project__

# if not _config_dir:
# 	config_dir = os.path.expanduser("~/.config")
# if appname:
# 	config_dir = os.path.join(config_dir, appname)
try:
	sys.path.insert(0, str(_config_dir))
	from config import Config  # noqa
except ModuleNotFoundError:
	logger.error(_config_dir)
	raise ModuleNotFoundError("config.py")



def main():
	start_date = get_last_playdatetime().date()
	end_date = (_run_dt - timedelta(days=1)).date()
	day_delta = timedelta(days=1)
	os.chdir(_data_dir)
	if start_date < end_date:
		for i in range(0, (end_date - start_date).days + 1):
			process_date = start_date + i * day_delta
			date_id = process_date.strftime("%Y%m%d")
			logdir = Path(date_id)
			if not logdir.exists():
				continue
			olddir = os.getcwd()
			os.chdir(logdir)
			merged_filename = f"merged_{date_id}.log.xz"
			if not exists(merged_filename):
				archive_filename = f"smplayer_logs_{date_id}.txz"
				archive_exists = exists(archive_filename)
				if archive_exists:
					logger.info(f"Expanding '{archive_filename}' . . .")
					cmd = f"tar -xvf {archive_filename}".split()
					result = run(cmd, capture_output=True, universal_newlines=True)
					for line in [x.rstrip() for x in result.stdout.splitlines()]:
						print(line)
				merge_logs(os.getcwd(), merged_filename)
			playinfo_filename = f"playinfo_{date_id}.csv.xz"
			df = extract_playinfo(process_date, merged_filename, playinfo_filename)

			rows = df.to_sql(tablename, con=engine, schema=schema,
							 if_exists="append", index=False)
			logger.info(f"Added {rows} rows to database for {process_date}")

			os.chdir(olddir)
			mtime = datetime.combine(process_date, datetime.max.time()).timestamp()
			os.utime(logdir, times=(mtime, mtime))
	return


@click.command()
@click.argument("log_date", default="today")
@click.option(
	"-f",
	"--force/--no-force",
	is_flag=True,
	default=False,
	help="Force data to be written (overwrite)",
)
def main_old(log_date, force):
	start_date = get_last_playdatetime().date()
	# yesterday = pendulum.yesterday().date()
	yesterday = (_run_dt - timedelta(days=1)).date()
	if log_date.lower() in ["today", "yesterday"]:
		log_date = yesterday
	# elif Config.REBUILD and log_date.lower() == "rebuild":
	# 	rebuild()
	else:
		try:
			log_date = parse(log_date).date()
		except ParserError as e:
			# fn_logger.exception(e)
			raise
	logger.info(f"Log Date: {log_date}")
	if log_date > _run_dt.date():
		raise ValueError("Future dates aren't processed.")
	# process_date = (_run_dt - timedelta(days=1)).date()
	# process_date = _run_dt.date()
	date_id = log_date.strftime("%Y%m%d")
	log_dir = _data_dir / date_id
	logger.info(f"Log Dir: {log_dir}")
	merged_filename = log_dir / f"merged_{date_id}.log.xz"
	if not exists(merged_filename):
		merge_logs(log_dir, merged_filename)
	playinfo_filename = log_dir / f"playinfo_{date_id}.csv.xz"

	df = extract_playinfo(log_date, merged_filename, playinfo_filename)
	# logger.debug(df)

	# ToDo: Add table dt_playhistory3
	rows = df.to_sql(tablename, con=engine, schema=schema,
					 if_exists="append", index=False)
	logger.info(f"Added {rows} rows to database for {log_date}")

	return


def init():
	logger.debug(f"Run Start: {__module__} v{__version__} {_run_dt}")
	"""rebuild = Config.REBUILD
	truncate_tables = Config.TRUNCATE_TABLES

	if Config.__app_config__ == "development" and schema == "new_media_library":
		if truncate_tables:
			sql = f"TRUNCATE TABLE {tablename};"
			with engine.connect() as conn:
				result = conn.execute(text(sql))
				conn.commit()
			do_nothing()"""
	return


def eoj():
	stop_dt = datetime.now().astimezone().replace(microsecond=0)
	duration = stop_dt.replace(microsecond=0) - _run_dt.replace(microsecond=0)
	logger.debug("Run Stop : %s  Duration: %s" % (stop_dt, duration))
	return


def do_nothing():
	pass


@contextmanager
def elapsed_timer():
	start = timer()
	elapser = lambda: timer() - start
	yield lambda: elapser()
	end = timer()
	elapser = lambda: end-start


# ToDo: Re-engineer dt_playhistory to include stop time
def extract_playinfo(play_date: datetime.date, merged_filename: str | Path, playinfofilename: str | Path) -> pd.DataFrame:
	# logger.debug(f"Play Date:  . . . . . . {play_date}")
	# logger.debug(f"Log Filename  . . . . . {merged_filename}")
	# logger.debug(f"PlayInfo Filename . . . {playinfofilename}")
	#
	# Extract media & time information from merged log file
	start_regex = r"Core::startMplayer: file:"
	stop_regex = r"Core::stopMplayer$"
	play_regex = r"Playlist::playItem:"
	patterns = "|".join([start_regex, stop_regex, play_regex])

	if type(playinfofilename) is not Path:
		playinfofilename = Path(playinfofilename)

	if str(merged_filename).endswith(".xz"):
		open = lzma.open
	else:
		open = builtins.open
	logger.debug(f"Reading {merged_filename} . . .")
	with open(merged_filename, "rt") as logfile:
		lines = [x.rstrip() for x in logfile.readlines()]
	# logger.debug(f"Line Count: {len(lines)}  {merged_filename}")
	# info_lines = [x for x in lines if re.search(start_regex, x)]
	info_lines = [x for x in lines if re.search(patterns, x)]
	# logger.debug(f"Info Lines: {len(info_lines)}  {merged_filename}")

	csv_rows = []
	seen_plays = []
	yr = play_date.year
	mo = play_date.month
	d = play_date.day

	last_play_dt = None
	# last_stop_dt = None
	last_filename = None
	for il in info_lines:
		parts = il.split()
		timepart = parts[0].lstrip("[").rstrip("]")
		h, m, s, ms = [int(x) for x in timepart.split(":")]
		dt = datetime(yr, mo, d, h, m, s, ms).astimezone()
		filename = " ".join(parts[3:-2]).strip('"')
		if filename:
			play_dt = dt
			stop_dt = None
			last_filename = filename
			last_play_dt = play_dt
		else:
			filename = last_filename
			stop_dt = dt
			last_stop_dt = stop_dt
			play_dt = last_play_dt
		if filename and stop_dt:
			# epochtime = play_dt.timestamp()
			play_time = stop_dt - play_dt
			# play_time1 = str(play_time)
			play_secs = play_time.total_seconds()
			if play_secs > 1800:
				do_nothing()
			if play_dt not in seen_plays:
				row = (play_date, play_dt.isoformat(" "), stop_dt.isoformat(" "), filename, play_secs, play_time)
				csv_rows.append(row)
				seen_plays.append(play_dt)
			else:
				do_nothing()

	# ToDo: Improve this logic Config options seem to be competing, and SAVE_SUMMARIES
	#       is too far down
	if csv_rows:
		if exists(playinfofilename) and Config.OVERWRITE_PLAYINFO_CSV:
			os.remove(playinfofilename)
		if not exists(playinfofilename):
			playinfofilename.parent.mkdir(exist_ok=True)
			logger.debug(f"Building DataFrame for {play_date} . . .")
			columns = "playdate playdatetime stopdatetime filename play_secs play_time".split()
			df = pd.DataFrame(csv_rows, columns=columns)
			try:
				df["playdatetime"] = pd.to_datetime(df["playdatetime"], format='ISO8601')
				df["stopdatetime"] = pd.to_datetime(df["stopdatetime"], format='ISO8601')
				df["play_time"] = pd.to_timedelta(df["play_time"])
			except Exception as e:
				logger.exception(e, file=sys.stderr)
				do_nothing()
			df["playdate"] = pd.to_datetime(df["playdate"]).dt.date
			df["play_time"] = df["play_time"].astype(str)
			# Sort the DataFrame
			df.sort_values(by=["playdatetime"], inplace=True)
			if Config.SAVE_SUMMARIES:
				# Save to tab-delimited file
				logger.info(f"Saving Summary to {playinfofilename} . . .")
				df.to_csv(playinfofilename, sep="\t", header=columns, index=False)
				mtime = getmtime(merged_filename)
				os.utime(playinfofilename, times=(mtime, mtime))
				os.utime(playinfofilename.parent, times=(mtime, mtime))
	return df


def get_last_playdatetime() -> datetime:
	sql = f"SELECT max(playdatetime) FROM {tablename};"
	with engine.connect() as conn:
		# rows = dict(conn.execute(sa.text(sql)).fetchone())["last_playdatetime"]
		playdatetime = conn.execute(sa.text(sql)).fetchone()[0]
	return playdatetime


def get_log_entries(filename: str | Path) -> list[str]:
	entries = []
	# Sample timestamp: [03:21:17:080]
	log_regex = r"^\[[012][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]{3}\]"
	if str(filename).endswith(".xz"):
		open = lzma.open
	else:
		open = builtins.open
	with open(filename, "rt") as logfile:
		lines = [x.rstrip() for x in logfile.readlines()]
	if lines[-1].endswith("AssStyles::save"):
		do_nothing()
	entry_lines = []
	for line in lines:
		# If line starts with a timestamp, start a new collection of line entries
		if re.search(log_regex, line):
			if entry_lines:
				entries.append("\n".join(entry_lines))
			entry_lines = [line,]
		else:
			entry_lines.append(line)
	# Get last entry
	if entry_lines:
		if len(entry_lines) > 1:
			do_nothing()
		entries.append("\n".join(entry_lines))
	return entries


# ToDo: Add multi-threading with the concurrent.futures module, like in ydl-fetch
def merge_logs(log_dir: str | Path, merged_filename: str | Path, archive_exists: bool=False):
	logger.debug(f"Merging Logs . . .")
	logfiles = sorted(glob(join(log_dir, "smplayer*log")), key=getmtime)
	last_mtime = getmtime(logfiles[-1])
	if exists(merged_filename):
		merged_entries = get_log_entries(merged_filename)
	else:
		merged_entries = []
	found_new_entries = False

	# We can use a with statement to ensure threads are cleaned up promptly
	max_errors = 3
	error_count = 0
	with concurrent.futures.ThreadPoolExecutor(max_workers=25) as executor:
		# Start the load operations and mark each future with its video data
		future_get_entries = {executor.submit(get_log_entries, l): l for l in logfiles}
		future_count = 0
		for future in concurrent.futures.as_completed(future_get_entries):
			future_count += 1
			if future_count > 1:
				sleep((0.1))
			logfile = future_get_entries[future]
			logger.debug(f"{future_count:4d}) {logfile}")
			try:
				data = future.result()
			except Exception as e:
				logger.exception(f"{logfile} {e}")
			else:
				new_entries = [x.rstrip() for x in data if x not in merged_entries]
			if new_entries:
				merged_entries.extend(new_entries)
				found_new_entries = True
				do_nothing()
	if found_new_entries:
		if str(merged_filename).endswith(".xz"):
			open = lzma.open
		else:
			open = builtins.open
		logger.debug(f"Writing to {merged_filename} . . .")
		with open(merged_filename, "wt") as fp:
			fp.writelines([f"{x}\n" for x in merged_entries])

	# If log files have already been archived, delete them
	for l in logfiles:
		try:
			os.remove(l)
			sleep(0.1)
		except IOError as ioe:
			logger.exception(f"{l} {ioe}")
			do_nothing()
	# Set modification time to that of the most recent log file
	os.utime(merged_filename, times=(last_mtime, last_mtime))
	return


if __name__ == "__main__":
	_run_dt = datetime.now().astimezone().replace(microsecond=0)
	_run_utc = _run_dt.astimezone(timezone.utc).replace(tzinfo=None)
	_fdate = _run_dt.strftime("%Y-%m-%d")
	_fdatetime = _run_dt.strftime("%Y%m%d_%H%M%S")

	# Configure Directories
	_data_dir = xdg.xdg_data_home() / appname
	_state_dir = xdg.xdg_state_home() / appname
	_logdir = _state_dir / "log"
	# Directories
	_cache_dir = xdg.xdg_cache_home() / __org__ / __project__
	_config_dir = xdg.xdg_config_home() / __org__ / __project__
	_data_dir = xdg.xdg_data_home() / __org__ / __project__
	_runtime_dir = xdg.xdg_runtime_dir() / __org__ / __project__
	_state_dir = xdg.xdg_state_home() / __org__ / __project__
	# Sub-Directories
	_log_dir = _state_dir / "log"

	# Configure Database
	engine = create_engine(Config.DATABASE_URL)
	schema = Config.DB_SCHEMA
	tablename = Config.TABLE_NAME

	# SET PosgreSQL search_path
	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit

	# Loguru Configuration (logger)
	_logfile = _logdir / f"{__module__}.log"
	_errfile = _logdir / f"{__module__}.err"

	logger.remove(0)
	logger.add(
		sys.stderr,
		backtrace=True,
		colorize=True,
		diagnose=True,
		format="<level>{level:8s} {function}:{line:03d}  {message}</level>",
		level="DEBUG",
	)
	logger.add(
		_logfile,
		colorize=False,
		compression="gz",
		format="<green>{time:%Y-%m-%d %H:%M:%S%z}</green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="TRACE",
	)
	logger.add(
		_errfile,
		colorize=False,
		compression="gz",
		format="<green>{time:%Y-%m-%d %H:%M:%S%z}></green> <level>{level:8s} {name}:{function}:{line:03d}  {message}</level>",
		rotation="10 MB",
		level="ERROR",
	)
	# logger.add("debug.log", rotation="10 MB", level="DEBUG")
	# logger.add("info.log", rotation="10 MB", level="INFO")
	# logger.add("warning.log", rotation="10 MB", level="WARNING")
	# logger.add("critical.log", rotation="10 MB", level="CRITICAL")


	init()
	main()
	eoj()
