#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# log_extractor - Friday, May 27, 2022
""" Extract timestamps and filenames from the collected logs (via log_collector) """
__version__ = '0.2.9-dev3'

import pandas as pd
import lzma, os, re, shutil, sys
from datetime import datetime, timedelta, timezone
from glob import glob
from os.path import exists, expanduser, join
from pathlib import Path
from xdg import XDG_DATA_HOME, XDG_RUNTIME_DIR

"""parentdir = Path(__file__).resolve().parent.parent
if parentdir not in sys.path:
	sys.path.insert(0, str(parentdir))

from config import Config"""

__MODULE__ = Path(__file__).resolve().stem


def main(args):
	"""
	Find log directories with new data and extract timestamps & filenames
	To reprocess a date in the last 10 days, simply remove it's Summary file
	"""
	# Date ID for today's data, to be ignored until tomorrow
	run_date_id = _run_dt.strftime("%Y%m%d")
	# if args:
	#	date_id = args[0]
	date_ids = []
	# Scan for directories in YYYYMMDD format
	date_regex = r"((19|20)\d{2}(0[1-9]|1[0-2])(0[1-9]|[12]\d|3[01]))"
	for subdir in list(_data_dir.iterdir())[-1000:]:
		date_id = subdir.name
		if date_id == run_date_id:
			continue
		if re.match(date_regex, date_id):
			summaryfilename = _data_dir / "Summaries" / f"{date_id}.csv.xz"
			if not summaryfilename.exists():
				print(f"Processing: {date_id} . . .")
				extract_loginfo(subdir, summaryfilename)
	return


def init():
	print("Run Start: %s" % _run_dt)
	return


def eoj():
	stop_dt = datetime.now().astimezone().replace(microsecond=0)
	duration = stop_dt.replace(microsecond=0) - _run_dt.replace(microsecond=0)
	print("Run Stop : %s  Duration: %s" % (stop_dt, duration))
	return


def do_nothing():
	pass


def extract_loginfo(logdir, summaryfilename):
	csvlines = []
	seen_lines = []
	# Extract media & time information from log files
	log_regex = r"Core::startMplayer: file:"
	# log_dir = _data_dir / date_id
	last_mtime = -1
	log_filenames = logdir.glob("**/*.log")
	for log_filename in log_filenames:
		log_dirname = log_filename.parent.name
		log_year = int(log_dirname[0:4])
		log_month = int(log_dirname[4:6])
		log_day = int(log_dirname[6:8])
		mtime = log_filename.stat().st_mtime
		if mtime > last_mtime:
			last_mtime = mtime
		with open(log_filename) as logfile:
			for line in [x.rstrip() for x in logfile.readlines()]:
				if re.search(log_regex, line):
					if line not in seen_lines:
						parts = line.split()
						timepart = parts[0].lstrip("[").rstrip("]")
						hh, mm, ss, ms = [int(x) for x in timepart.split(":")]
						play_dt = datetime(log_year, log_month, log_day, hh, mm, ss, ms)
						# play_date = play_dt.date()
						# iso = play_dt.isoformat(" ")
						# ts = play_dt.timestamp()
						filename = " ".join(parts[3:-2]).strip('"')
						csvlines.append((play_dt.timestamp(), play_dt.isoformat(" "), filename))
						seen_lines.append(line)
		# Compress file
		lzfilename = f"{filename}.xz"
		with open(filename, "rb") as fin, lzma.open(lzfilename, "wb") as fout:
			# Reads the file by chunks to avoid exhausting memory
			shutil.copyfileobj(fin, fout)
		mtime = os.path.getmtime(filename)
		os.utime(lzfilename, (mtime, mtime))

	if csvlines:
		if not summaryfilename.exists():
			print(f"Found {len(csvlines):,d} lines to process")
			summaryfilename.parent.mkdir(exist_ok=True)
			columns = ["epochtime", "playdate", "filename"]
			df = pd.DataFrame(csvlines, columns=columns)
			# Save to tab-delimited file
			df.to_csv(summaryfilename, "\t", header=columns, index=False)
			os.utime(summaryfilename, times=(last_mtime, last_mtime))
			os.utime(logdir, times=(last_mtime, last_mtime))
	return


if __name__ == '__main__':
	_run_dt = datetime.now().astimezone().replace(microsecond=0)
	_run_utc = _run_dt.astimezone(timezone.utc).replace(tzinfo=None)
	_fdate = _run_dt.strftime("%Y-%m-%d")
	_fdatetime = _run_dt.strftime("%Y%m%d_%H%M%S")

	# _data_dir = Path("~/.local/share/MusicHistory").expanduser()
	_data_dir = Path(XDG_DATA_HOME) / "MusicHistory"

	args = None
	if len(sys.argv) > 1:
		args = sys.argv[1:]
	init()
	main(args)
	eoj()
