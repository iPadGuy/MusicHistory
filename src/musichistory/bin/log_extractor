#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# log_extractor_2 - Tuesday, December 10, 2024
""" Extract & analyze timestamps and filenames from collected SMPlayer logs (via log_collector)
	and generate Top XXX lists """
__version__ = "0.6.30-dev15"

import builtins
import lzma
import os
import re
import shutil
import sys
import tarfile
from datetime import datetime, timedelta, timezone
from os.path import basename, dirname, getmtime, getsize
from pathlib import Path
from time import sleep

import pandas as pd
import sqlalchemy as sa
import xdg
from sqlalchemy import create_engine
# from tabulate import tabulate
# from xdg import XDG_DATA_HOME, XDG_CONFIG_HOME

appname = "MusicHistory"
config_dir = xdg.xdg_config_home()
if not config_dir:
	config_dir = os.path.expanduser("~/.config")
if appname:
	config_dir = os.path.join(config_dir, appname)
try:
	sys.path.insert(0, config_dir)
except ModuleNotFoundError:
	raise ModuleNotFoundError("config.py")
from config import Config  # noqa

__MODULE__ = Path(__file__).resolve().stem


def main():
	# Under normal conditions, start_date should be at least two days ago
	start_date = fetch_last_playdatetime().date()
	# start_date = datetime(2022, 12, 5).date()
	# Since we're only processing complete days, end_date is yesterday
	end_date = (_run_dt - timedelta(days=1)).date()
	day_delta = timedelta(days=1)
	# tablename = "dt_playinfo"
	os.chdir(_data_dir)
	if start_date < end_date:
		for i in range(1, (end_date - start_date).days + 1):
			process_date = start_date + i * day_delta
			date_id = process_date.strftime("%Y%m%d")
			logdir = Path(date_id)
			##### 2025-02-21 PAL - Remove empty log files - BEGIN
			for entry in [x for x in os.scandir(logdir) if x.is_file()]:
				if getsize(entry.path) == 0:
					os.remove(entry.path)
			##### 2025-02-21 PAL - Remove empty log files - END

			##### 2022-12-06 PAL - Merge Log Files - BEGIN
			merged_filename = logdir / f"merged_{date_id}.log.xz"
			playinfo_filename = logdir / f"playinfo_{date_id}.csv.xz"
			logfiles = [
				f for f in os.scandir(logdir) if f.is_file()
					and f.name.startswith("smplayer")
					and f.name.endswith(".log")
			]
			# summaryfilename = logdir / f"summary_{date_id}.csv.xz"
			# tarfilename = logdir / f"smplayer_logs_{date_id}.txz"

			if playinfo_filename.exists() and not logfiles:
				continue
			elif logfiles:
				logfiles = sorted(logfiles, key=getmtime)
				last_mtime = getmtime(logfiles[-1])
				if not merged_filename.exists():
					# Create merged file from the first uncompressed log file
					with open(logfiles[0], "rb") as fin, lzma.open(
						merged_filename, "wb"
					) as fout:
						# Reads the file by chunks to avoid exhausting memory
						shutil.copyfileobj(fin, fout)
				total_count = 0
				for i in range(0, len(logfiles) - 1):
					j = i + 1
					count = find_new_lines(
						logfiles[i].path, logfiles[j].path, merged_filename
					)
					total_count += count
					do_nothing()
				# Set modification time to that of the most recent log file
				os.utime(merged_filename, times=(last_mtime, last_mtime))
			# ToDo: Replace summaryfilename and extract_loginfo()
			##### 2022-12-06 PAL - Merge Log Files - END

			# summaryfilename = logdir / f"summary_{date_id}.csv.xz"
			# tarfilename = logdir / f"smplayer_logs_{date_id}.txz"

			if not playinfo_filename.exists():
				# Only process log files that don't have a matching playlist file
				print(f"Processing: {process_date} . . .")
				# df = extract_loginfo(logdir, summaryfilename, tarfilename)
				df = extract_playinfo(process_date, merged_filename, playinfo_filename)
			else:
				print(f"Loading: '{playinfo_filename}'")
				df = pd.read_csv(playinfo_filename, sep="\t", parse_dates=["playdatetime"])
			# Fetch last row from database
			columns = [
				"epochtime",
				"playdate",
				"playdatetime",
				"filename",
				"play_secs",
				"play_time",
			]
			cols = ", ".join(columns)
			sql = f"SELECT {cols} FROM {tablename} ORDER BY epochtime DESC LIMIT 1;"
			last_df = pd.read_sql_query(sql, con=engine)

			# Merge last row from database
			df = pd.concat([last_df, df], ignore_index=True)
			df.drop_duplicates(subset=["epochtime"], inplace=True)

			# Calculate durations / play times
			df["play_secs"] = df.shift(-1)["epochtime"] - df["epochtime"]
			df["play_time"] = df.shift(-1)["playdatetime"] - df["playdatetime"]
			# df["play_time"].fillna(timedelta(0), inplace=True)
			# df["play_time"] = df["play_time"].astype(str)

			# Drop last row (was added, above)
			df.drop(0, inplace=True)

			# Calculate play time for last row, using mtime of playinfo file
			# summary_mtime = summaryfilename.stat().st_mtime
			# summary_dt = datetime.fromtimestamp(summary_mtime).astimezone()
			playinfo_mtime = getmtime(playinfo_filename)
			playinfo_dt = datetime.fromtimestamp(playinfo_mtime).astimezone()

			""" 2024-Oct-15 PAL - FutureWarning for Pandas 3.0
			OLD:

			df["col"][row_indexer] = value

			NEW:

			df.loc[row_indexer, "col"] = values
			"""
			""" Old:
			# Prevent 'SettingWithCopyWarning' message from appearing
			pd.options.mode.chained_assignment = None
			df["play_secs"].iloc[-1] = summary_mtime - df["epochtime"].iloc[-1]
			df["play_time"].iloc[-1] = summary_dt - df["playdatetime"].iloc[-1]
			df["play_time"] = df["play_time"].astype(str)
			"""
			df.loc[df.index[-1], "play_secs"] = playinfo_mtime - df["epochtime"].iloc[-1]
			df.loc[df.index[-1], "play_time"] = playinfo_dt - df["playdatetime"].iloc[-1]
			df["play_time"] = df["play_time"].astype(str)

			# See if epochtimes already exist
			"""epochtimes_list = list(df["epochtime"].values)
			where_clause = "epochtime IN (%s)" % ",".join([str(x) for x in epochtimes_list])
			sql = f"SELECT epochtime FROM {schema}.{tablename} WHERE {where_clause};"
			existing_df = pd.read_sql_query(sql, con=engine)
			if not existing_df["epochtime"].empty:
				df = df.loc[df["epochtime"] != existing_df["epochtime"]]"""
			rows = df.to_sql(tablename, con=engine, schema=schema,
							 if_exists="append", index=False)
			print(f"Added {rows} rows to database")
			do_nothing()
	return


def init():
	print(f"Run Start: {__MODULE__} v{__version__} {_run_dt}")
	return


def eoj():
	stop_dt = datetime.now().astimezone().replace(microsecond=0)
	duration = stop_dt.replace(microsecond=0) - _run_dt.replace(microsecond=0)
	print("Run Stop : %s  Duration: %s" % (stop_dt, duration))
	return


def do_nothing():
	pass


def extract_loginfo_old(
	logdir: str | Path, summaryfilename: str | Path, tarfilename: str | Path
) -> pd.DataFrame:
	df = None
	csv_rows = []
	seen_lines = []
	# Extract media & time information from log files
	log_regex = r"Core::startMplayer: file:"
	# log_dir = _data_dir / date_id
	last_mtime = -1
	# 2022-12-07 PAL - Switching from glob() to scandir() - BEGIN
	log_filenames = [
		x
		for x in os.scandir(logdir)
		if x.is_file() and x.name.startswith("smplayer") and x.name.endswith(".log")
	]
	log_filenames = sorted(log_filenames, key=getmtime)
	last_filename = log_filenames[-1]
	last_mtime = getmtime(last_filename)
	with tarfile.open(tarfilename, "w:xz") as tar:
		for log_filename in log_filenames:
			date_id = basename(dirname(log_filename.path))
			yyyy = int(date_id[0:4])
			mm = int(date_id[4:6])
			dd = int(date_id[6:8])
			with open(log_filename) as logfile:
				for line in [x.rstrip() for x in logfile.readlines()]:
					if re.search(log_regex, line):
						if line not in seen_lines:
							parts = line.split()
							timepart = parts[0].lstrip("[").rstrip("]")
							hh, mm, ss, ms = [int(x) for x in timepart.split(":")]
							play_dt = datetime(
								yyyy, mm, dd, hh, mm, ss, ms
							).astimezone()
							play_date = play_dt.date()
							filename = " ".join(parts[3:-2]).strip('"')
							csv_rows.append(
								(
									play_dt.timestamp(),
									play_date,
									play_dt.isoformat(" "),
									filename,
								)
							)
							seen_lines.append(line)
			# Compress log file
			# lzfilename = f"{log_filename}.xz"
			# with open(log_filename, "rb") as fin, lzma.open(lzfilename, "wb") as fout:
			# Reads the file by chunks to avoid exhausting memory
			# 	shutil.copyfileobj(fin, fout)
			# sleep(0.1)
			# Set modification time to that of the last log file
			# mtime = os.path.getmtime(log_filename)
			# os.utime(lzfilename, (mtime, mtime))
			# Add log file to archive
			tar.add(log_filename)
			sleep(0.1)
			# Delete original log file
			os.remove(log_filename)
	# 2022-12-07 PAL - Switching from glob() to scandir() - END
	sleep(0.1)
	os.utime(tarfilename, times=(last_mtime, last_mtime))
	# 2022-12-09 PAL - The last file still isn't being deleted . . .
	if os.path.exists(last_filename):
		os.remove(last_filename)

	# Process rows from CSV log files
	if csv_rows:
		if not summaryfilename.exists():
			print(f"Found {len(csv_rows):,d} lines to process")
			summaryfilename.parent.mkdir(exist_ok=True)
			columns = ["epochtime", "playdate", "playdatetime", "filename"]
			df = pd.DataFrame(csv_rows, columns=columns)
			df["playdatetime"] = pd.to_datetime(df["playdatetime"])
			df["playdate"] = pd.to_datetime(df["playdate"]).dt.date
			if Config.SAVE_SUMMARIES:
				# Save to tab-delimited file
				df.to_csv(summaryfilename, sep="\t", header=columns, index=False)
				os.utime(summaryfilename, times=(last_mtime, last_mtime))
				os.utime(logdir, times=(last_mtime, last_mtime))
	return df


def extract_playinfo(play_date: datetime.date, merged_filename: str | Path, playinfofilename: str | Path) -> pd.DataFrame:
	# print(f"Play Date:  . . . . . . {play_date}")
	# print(f"Log Filename  . . . . . {merged_filename}")
	# print(f"PlayInfo Filename . . . {playinfofilename}")
	#
	# Extract media & time information from merged log file
	log_regex = r"Core::startMplayer: file:"

	if str(merged_filename).endswith(".xz"):
		open = lzma.open
	else:
		open = builtins.open
	with open(merged_filename, "rt") as logfile:
		lines = [x.rstrip() for x in logfile.readlines()]
	# print(f"Line Count: {len(lines)}  {merged_filename}")
	info_lines = [x for x in lines if re.search(log_regex, x)]
	# print(f"Info Lines: {len(info_lines)}  {merged_filename}")

	csv_rows = []
	date_id = play_date.strftime("%Y%m%d")
	# print(f"Date ID: {date_id}")
	yr = play_date.year
	mo = play_date.month
	d = play_date.day
	# yyyy = int(date_id[0:4])
	# mm = int(date_id[4:6])
	# dd = int(date_id[6:8])

	for il in info_lines:
		# print(il)
		parts = il.split()
		timepart = parts[0].lstrip("[").rstrip("]")
		h, m, s, ms = [int(x) for x in timepart.split(":")]
		play_dt = datetime(yr, mo, d, h, m, s, ms).astimezone()
		play_date1 = play_dt.date()
		filename = " ".join(parts[3:-2]).strip('"')
		row = (play_dt.timestamp(), play_date, play_dt.isoformat(" "), filename,)
		csv_rows.append(row)

	if csv_rows:
		if not playinfofilename.exists():
			playinfofilename.parent.mkdir(exist_ok=True)
			columns = ["epochtime", "playdate", "playdatetime", "filename"]
			df = pd.DataFrame(csv_rows, columns=columns)
			try:
				df["playdatetime"] = pd.to_datetime(df["playdatetime"], format='ISO8601')
			except Exception as e:
				print(e, file=sys.stderr)
				do_nothing()
			df["playdate"] = pd.to_datetime(df["playdate"]).dt.date
			if Config.SAVE_SUMMARIES:
				# Save to tab-delimited file
				df.to_csv(playinfofilename, sep="\t", header=columns, index=False)
				mtime = getmtime(merged_filename)
				os.utime(playinfofilename, times=(mtime, mtime))
				os.utime(playinfofilename.parent, times=(mtime, mtime))

	return df


def fetch_last_playdatetime() -> datetime:
	# tablename = "dt_playinfo"
	sql = f"SELECT max(playdatetime) FROM {tablename};"
	with engine.connect() as conn:
		# rows = dict(conn.execute(sa.text(sql)).fetchone())["last_playdatetime"]
		playdatetime = conn.execute(sa.text(sql)).fetchone()[0]
	return playdatetime


def find_new_lines(
	older_filename: str | Path, newer_filename: str | Path, merged_filename: str | Path
) -> int:
	new_lines = []
	lines_to_compare = 100
	# Get the correct open() function
	if older_filename.endswith(".xz"):
		open1 = lzma.open
	else:
		open1 = builtins.open
	if newer_filename.endswith(".xz"):
		open2 = lzma.open
	else:
		open2 = builtins.open

	with open1(older_filename, "rt") as f1, open2(newer_filename, "rt") as f2:
		lines1 = f1.readlines()
		if not lines1:
			do_nothing()
		lines2 = f2.readlines()
		if not lines2:
			do_nothing()
		line_count1 = len(lines1)
		line_count2 = len(lines2)
		if line_count2 > line_count1:
			if lines1[-1] == lines2[line_count1 - 1]:
				# Top part of files match
				new_lines = lines2[line_count1:]
				do_nothing()
			else:
				# New file has all-new lines (should rarely happen, if ever)
				new_lines = lines2
				do_nothing()
		elif line_count1 == line_count2:
			# Same file? (Shouldn't happen)
			do_nothing()
		else:
			# Top part of files are different - SMPlayer was probably restarted
			new_lines = lines2
			do_nothing()
		if new_lines:
			if str(merged_filename).endswith(".xz"):
				open3 = lzma.open
			else:
				open3 = builtins.open
			with open3(merged_filename, "at") as fout:
				fout.writelines(new_lines)
		do_nothing()
	return len(new_lines)


if __name__ == "__main__":
	_run_dt = datetime.now().astimezone().replace(microsecond=0)
	_run_utc = _run_dt.astimezone(timezone.utc).replace(tzinfo=None)
	_fdate = _run_dt.strftime("%Y-%m-%d")
	_fdatetime = _run_dt.strftime("%Y%m%d_%H%M%S")

	# Configure Directories
	_data_dir = xdg.xdg_data_home() / appname

	# Configure Database
	engine = create_engine(Config.DATABASE_URL)
	schema = Config.DB_SCHEMA
	tablename = Config.TABLE_NAME

	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit

	init()
	main()
	eoj()
