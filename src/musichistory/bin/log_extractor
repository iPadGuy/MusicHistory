#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# log_extractor - Friday, May 27, 2022
""" Extract & analyze timestamps and filenames from collected SMPlayer logs (via log_collector)
	and generate Top XXX lists """
__version__ = "0.5.21-dev8"

import builtins
import lzma
import os
import re
import shutil
import sys
import tarfile
from datetime import datetime, timedelta, timezone
from os.path import basename, dirname, getmtime
from pathlib import Path
from time import sleep

import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
# from tabulate import tabulate
from xdg import XDG_DATA_HOME, XDG_CONFIG_HOME

appname = "MusicHistory"
config_dir = XDG_CONFIG_HOME
if not config_dir:
	config_dir = os.path.expanduser("~/.config")
if appname:
	config_dir = os.path.join(config_dir, appname)
try:
	sys.path.insert(0, config_dir)
except ModuleNotFoundError:
	raise ModuleNotFoundError("config.py")
from config import Config  # noqa

__MODULE__ = Path(__file__).resolve().stem


def main():
	# Under normal conditions, start_date should be at least two days ago
	start_date = fetch_last_playdatetime().date()
	# start_date = datetime(2022, 12, 5).date()
	# Since we're only processing complete days, end_date is yesterday
	end_date = (_run_dt - timedelta(days=1)).date()
	day_delta = timedelta(days=1)
	# tablename = "dt_playinfo"
	os.chdir(_data_dir)
	if start_date < end_date:
		for i in range(1, (end_date - start_date).days + 1):
			process_date = start_date + i * day_delta
			date_id = process_date.strftime("%Y%m%d")
			logdir = Path(date_id)
			##### 2022-12-06 PAL - Merge Log Files - BEGIN
			merged_filename = logdir / f"merged_{date_id}.log.xz"
			logfiles = [
				f
				for f in os.scandir(logdir)
				if f.is_file()
				and f.name.startswith("smplayer")
				and f.name.endswith(".log")
			]
			if not logfiles:
				continue
			logfiles = sorted(logfiles, key=getmtime)
			last_mtime = getmtime(logfiles[-1])
			# Create merged file from the first uncompressed log file
			with open(logfiles[0], "rb") as fin, lzma.open(
				merged_filename, "wb"
			) as fout:
				# Reads the file by chunks to avoid exhausting memory
				shutil.copyfileobj(fin, fout)
			total_count = 0
			for i in range(0, len(logfiles) - 1):
				j = i + 1
				count = find_new_lines(
					logfiles[i].path, logfiles[j].path, merged_filename
				)
				total_count += count
				do_nothing()
			# Set modification time to that of the most recent log file
			os.utime(merged_filename, times=(last_mtime, last_mtime))
			# This will replace summaryfilename and extract_loginfo()
			playinfo_filename = logdir / f"playinfo_{date_id}.csv.xz"
			extract_playinfo(merged_filename, playinfo_filename)
			##### 2022-12-06 PAL - Merge Log Files - END

			summaryfilename = logdir / f"summary_{date_id}.csv.xz"
			tarfilename = logdir / f"smplayer_logs_{date_id}.txz"

			if not summaryfilename.exists():
				# Only process log files that don't have a matching summary file
				print(f"Processing: {process_date} . . .")
				df = extract_loginfo(logdir, summaryfilename, tarfilename)
			else:
				print(f"Loading: '{summaryfilename}'")
				df = pd.read_csv(
					summaryfilename, sep="\t", parse_dates=["playdatetime"]
				)
			# Fetch last row from database
			columns = [
				"epochtime",
				"playdate",
				"playdatetime",
				"filename",
				"play_secs",
				"play_time",
			]
			cols = ", ".join(columns)
			sql = f"SELECT {cols} FROM {tablename} ORDER BY epochtime DESC LIMIT 1;"
			last_df = pd.read_sql_query(sql, con=engine)

			# Merge last row from database
			df = pd.concat([last_df, df], ignore_index=True)

			# Calculate durations / play times
			df["play_secs"] = df.shift(-1)["epochtime"] - df["epochtime"]
			df["play_time"] = df.shift(-1)["playdatetime"] - df["playdatetime"]
			# df["play_time"].fillna(timedelta(0), inplace=True)
			# df["play_time"] = df["play_time"].astype(str)

			# Drop last row (was added, above)
			df.drop(0, inplace=True)

			# Calculate play time for last row, using mtime of summary file
			summary_mtime = summaryfilename.stat().st_mtime
			summary_dt = datetime.fromtimestamp(summary_mtime).astimezone()

			""" 2024-Oct-15 PAL - FutureWarning for Pandas 3.0
			OLD:

			df["col"][row_indexer] = value

			NEW:

			df.loc[row_indexer, "col"] = values
			"""
			""" Old:
			# Prevent 'SettingWithCopyWarning' message from appearing
			pd.options.mode.chained_assignment = None
			df["play_secs"].iloc[-1] = summary_mtime - df["epochtime"].iloc[-1]
			df["play_time"].iloc[-1] = summary_dt - df["playdatetime"].iloc[-1]
			df["play_time"] = df["play_time"].astype(str)
			"""
			df.loc[df.index[-1], "play_secs"] = summary_mtime - df["epochtime"].iloc[-1]
			df.loc[df.index[-1], "play_time"] = summary_dt - df["playdatetime"].iloc[-1]
			df["play_time"] = df["play_time"].astype(str)

			# See if epochtimes already exist
			"""epochtimes_list = list(df["epochtime"].values)
			where_clause = "epochtime IN (%s)" % ",".join([str(x) for x in epochtimes_list])
			sql = f"SELECT epochtime FROM {schema}.{tablename} WHERE {where_clause};"
			existing_df = pd.read_sql_query(sql, con=engine)
			if not existing_df["epochtime"].empty:
				df = df.loc[df["epochtime"] != existing_df["epochtime"]]"""
			rows = df.to_sql(
				tablename, con=engine, schema=schema, if_exists="append", index=False
			)
			print(f"Added {rows} rows to database")
			do_nothing()
	return


def init():
	print(f"Run Start: {__MODULE__} v{__version__} {_run_dt}")
	return


def eoj():
	stop_dt = datetime.now().astimezone().replace(microsecond=0)
	duration = stop_dt.replace(microsecond=0) - _run_dt.replace(microsecond=0)
	print("Run Stop : %s  Duration: %s" % (stop_dt, duration))
	return


def do_nothing():
	pass


def extract_loginfo(
	logdir: str | Path, summaryfilename: str | Path, tarfilename: str | Path
) -> pd.DataFrame:
	df = None
	csvlines = []
	seen_lines = []
	# Extract media & time information from log files
	log_regex = r"Core::startMplayer: file:"
	# log_dir = _data_dir / date_id
	last_mtime = -1
	# 2022-12-07 PAL - Switching from glob() to scandir() - BEGIN
	log_filenames = [
		x
		for x in os.scandir(logdir)
		if x.is_file() and x.name.startswith("smplayer") and x.name.endswith(".log")
	]
	log_filenames = sorted(log_filenames, key=getmtime)
	last_filename = log_filenames[-1]
	last_mtime = getmtime(last_filename)
	with tarfile.open(tarfilename, "w:xz") as tar:
		for log_filename in log_filenames:
			date_id = basename(dirname(log_filename.path))
			log_year = int(date_id[0:4])
			log_month = int(date_id[4:6])
			log_day = int(date_id[6:8])
			with open(log_filename) as logfile:
				for line in [x.rstrip() for x in logfile.readlines()]:
					if re.search(log_regex, line):
						if line not in seen_lines:
							parts = line.split()
							timepart = parts[0].lstrip("[").rstrip("]")
							hh, mm, ss, ms = [int(x) for x in timepart.split(":")]
							play_dt = datetime(
								log_year, log_month, log_day, hh, mm, ss, ms
							).astimezone()
							play_date = play_dt.date()
							filename = " ".join(parts[3:-2]).strip('"')
							csvlines.append(
								(
									play_dt.timestamp(),
									play_date,
									play_dt.isoformat(" "),
									filename,
								)
							)
							seen_lines.append(line)
			# Compress log file
			# lzfilename = f"{log_filename}.xz"
			# with open(log_filename, "rb") as fin, lzma.open(lzfilename, "wb") as fout:
			# Reads the file by chunks to avoid exhausting memory
			# 	shutil.copyfileobj(fin, fout)
			# sleep(0.1)
			# Set modification time to that of the last log file
			# mtime = os.path.getmtime(log_filename)
			# os.utime(lzfilename, (mtime, mtime))
			# Add log file to archive
			tar.add(log_filename)
			sleep(0.1)
			# Delete original log file
			os.remove(log_filename)
	# 2022-12-07 PAL - Switching from glob() to scandir() - END
	sleep(0.1)
	os.utime(tarfilename, times=(last_mtime, last_mtime))
	# 2022-12-09 PAL - The last file still isn't being deleted . . .
	if os.path.exists(last_filename):
		os.remove(last_filename)

	# Process rows from CSV log files
	if csvlines:
		if not summaryfilename.exists():
			print(f"Found {len(csvlines):,d} lines to process")
			summaryfilename.parent.mkdir(exist_ok=True)
			columns = ["epochtime", "playdate", "playdatetime", "filename"]
			df = pd.DataFrame(csvlines, columns=columns)
			df["playdatetime"] = pd.to_datetime(df["playdatetime"])
			df["playdate"] = pd.to_datetime(df["playdate"]).dt.date
			if Config.SAVE_SUMMARIES:
				# Save to tab-delimited file
				df.to_csv(summaryfilename, sep="\t", header=columns, index=False)
				os.utime(summaryfilename, times=(last_mtime, last_mtime))
				os.utime(logdir, times=(last_mtime, last_mtime))
	return df


def fetch_last_playdatetime() -> datetime:
	# tablename = "dt_playinfo"
	sql = f"SELECT max(playdatetime) FROM {tablename};"
	with engine.connect() as conn:
		# rows = dict(conn.execute(sa.text(sql)).fetchone())["last_playdatetime"]
		playdatetime = conn.execute(sa.text(sql)).fetchone()[0]
	return playdatetime


def find_new_lines(
	older_filename: str | Path, newer_filename: str | Path, merged_filename: str | Path
) -> int:
	new_lines = []
	lines_to_compare = 100
	# Get the correct open() function
	if older_filename.endswith(".xz"):
		open1 = lzma.open
	else:
		open1 = builtins.open
	if newer_filename.endswith(".xz"):
		open2 = lzma.open
	else:
		open2 = builtins.open

	with open1(older_filename, "rt") as f1, open2(newer_filename, "rt") as f2:
		lines1 = f1.readlines()
		lines2 = f2.readlines()
		line_count1 = len(lines1)
		line_count2 = len(lines2)
		if line_count2 > line_count1:
			if lines1[-1] == lines2[line_count1 - 1]:
				# Top part of files match
				new_lines = lines2[line_count1:]
				do_nothing()
			else:
				# New file has all-new lines (should rarely happen, if ever)
				new_lines = lines2
				do_nothing()
		elif line_count1 == line_count2:
			# Same file? (Shouldn't happen)
			do_nothing()
		else:
			# Top part of files are different - SMPlayer was probably restarted
			new_lines = lines2
			do_nothing()
		if new_lines:
			if str(merged_filename).endswith(".xz"):
				open3 = lzma.open
			else:
				open3 = builtins.open
			with open3(merged_filename, "at") as fout:
				fout.writelines(new_lines)
		do_nothing()
	return len(new_lines)


def extract_playinfo(logfilename: str | Path, playinfofilename: str | Path):

	return


if __name__ == "__main__":
	_run_dt = datetime.now().astimezone().replace(microsecond=0)
	_run_utc = _run_dt.astimezone(timezone.utc).replace(tzinfo=None)
	_fdate = _run_dt.strftime("%Y-%m-%d")
	_fdatetime = _run_dt.strftime("%Y%m%d_%H%M%S")

	# Configure Directories
	_data_dir = Path(XDG_DATA_HOME) / appname

	# Configure Database
	engine = create_engine(Config.DATABASE_URL)
	schema = Config.DB_SCHEMA
	tablename = Config.TABLE_NAME

	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit

	init()
	main()
	eoj()
