#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# log_extractor - Friday, May 27, 2022
""" Extract & analyze timestamps and filenames from collected SMPlayer logs (via log_collector)
    and generate Top XXX lists """
__version__ = "0.3.12-dev6"

import lzma
import os
import re
import shutil
import sys
from datetime import datetime, timezone
from pathlib import Path

import pandas as pd
from sqlalchemy import create_engine
from tabulate import tabulate
from xdg import XDG_DATA_HOME, XDG_CONFIG_HOME

appname = "MusicHistory"
config_dir = XDG_CONFIG_HOME
if not config_dir:
    config_dir = os.path.expanduser("~/.config")
if appname:
    config_dir = os.path.join(config_dir, appname)
try:
    sys.path.insert(0, config_dir)
except ModuleNotFoundError:
    raise ModuleNotFoundError("config.py")
from config import Config  # noqa

__MODULE__ = Path(__file__).resolve().stem


def main():
    """
    Find log directories with new data and extract timestamps & filenames
    To reprocess a date in the last 10 days, simply remove it's Summary file
    """
    # Date ID for today's data, to be ignored until tomorrow
    run_date_id = _run_dt.strftime("%Y%m%d")
    # if args:
    # 	date_id = args[0]
    # date_ids = []
    tablename = "dt_playinfo"
    # tablename = "dt_log_analyzer"
    # Scan for directories in YYYYMMDD format
    date_regex = r"((19|20)\d{2}(0[1-9]|1[0-2])(0[1-9]|[12]\d|3[01]))"
    for subdir in list(_data_dir.iterdir())[-10:]:
        if len(list(subdir.iterdir())) == 0:
            continue
        date_id = subdir.name
        if date_id == run_date_id:
            continue
        if re.match(date_regex, date_id):
            summaryfilename = _data_dir / "Summaries" / f"{date_id}.csv.xz"
            if not summaryfilename.exists():
                # Only process log files that don't have a matching summary file
                print(f"Processing: {date_id} . . .")
                df = extract_loginfo(subdir, summaryfilename)
                epochtimes_list = list(df["epochtime"].values)
                # See if epochtimes already exist
                where_clause = "epochtime IN (%s)" % ",".join([str(x) for x in epochtimes_list])
                sql = f"SELECT epochtime FROM {schema}.{tablename} WHERE {where_clause};"
                existing_df = pd.read_sql_query(sql, con=engine)
                if not existing_df["epochtime"].empty:
                    df = df.loc[df["epochtime"] != existing_df["epochtime"]]
                rows = df.to_sql(tablename, con=engine, index=False, if_exists="append", schema=schema)
                print(f"Added {rows} rows to database")
                do_nothing()
    return


def init():
    print("Run Start: %s" % _run_dt)
    return


def eoj():
    stop_dt = datetime.now().astimezone().replace(microsecond=0)
    duration = stop_dt.replace(microsecond=0) - _run_dt.replace(microsecond=0)
    print("Run Stop : %s  Duration: %s" % (stop_dt, duration))
    return


def do_nothing():
    pass


def extract_loginfo(logdir, summaryfilename):
    df = None
    csvlines = []
    seen_lines = []
    # Extract media & time information from log files
    log_regex = r"Core::startMplayer: file:"
    # log_dir = _data_dir / date_id
    last_mtime = -1
    log_filenames = logdir.glob("**/*.log")
    for log_filename in log_filenames:
        log_dirname = log_filename.parent.name
        log_year = int(log_dirname[0:4])
        log_month = int(log_dirname[4:6])
        log_day = int(log_dirname[6:8])
        mtime = log_filename.stat().st_mtime
        if mtime > last_mtime:
            last_mtime = mtime
        with open(log_filename) as logfile:
            for line in [x.rstrip() for x in logfile.readlines()]:
                if re.search(log_regex, line):
                    if line not in seen_lines:
                        parts = line.split()
                        timepart = parts[0].lstrip("[").rstrip("]")
                        hh, mm, ss, ms = [int(x) for x in timepart.split(":")]
                        play_dt = datetime(log_year, log_month, log_day, hh, mm, ss, ms).astimezone()
                        play_date = play_dt.date()
                        # iso = play_dt.isoformat(" ")
                        # ts = play_dt.timestamp()
                        filename = " ".join(parts[3:-2]).strip('"')
                        csvlines.append(
                            (play_dt.timestamp(), play_date, play_dt.isoformat(" "), filename)
                        )
                        seen_lines.append(line)
        # Compress file
        lzfilename = f"{log_filename}.xz"
        with open(log_filename, "rb") as fin, lzma.open(lzfilename, "wb") as fout:
            # Reads the file by chunks to avoid exhausting memory
            shutil.copyfileobj(fin, fout)
        mtime = os.path.getmtime(log_filename)
        os.utime(lzfilename, (mtime, mtime))

    if csvlines:
        if not summaryfilename.exists():
            print(f"Found {len(csvlines):,d} lines to process")
            summaryfilename.parent.mkdir(exist_ok=True)
            columns = ["epochtime", "playdate", "playdatetime", "filename"]
            df = pd.DataFrame(csvlines, columns=columns)
            df["playdatetime"] = pd.to_datetime(df["playdatetime"])
            df["playdate"] = pd.to_datetime(df["playdate"]).dt.date
            if Config.SAVE_SUMMARIES:
                # Save to tab-delimited file
                df.to_csv(summaryfilename, "\t", header=columns, index=False)
                os.utime(summaryfilename, times=(last_mtime, last_mtime))
                os.utime(logdir, times=(last_mtime, last_mtime))
    return df


if __name__ == "__main__":
    _run_dt = datetime.now().astimezone().replace(microsecond=0)
    _run_utc = _run_dt.astimezone(timezone.utc).replace(tzinfo=None)
    _fdate = _run_dt.strftime("%Y-%m-%d")
    _fdatetime = _run_dt.strftime("%Y%m%d_%H%M%S")

    _data_dir = Path(XDG_DATA_HOME) / appname
    engine = create_engine(Config.DATABASE_URL)
    schema = Config.DB_SCHEMA

    init()
    main()
    eoj()
