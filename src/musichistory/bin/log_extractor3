#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# log_extractor3 - Tuesday, March 18, 2025
""" Extract & analyze timestamps and filenames from collected SMPlayer logs (via log_collector)
	and generate Top XXX lists """
__version__ = "0.7.00-dev34"

import builtins
import lzma
import os
import re
import shutil
import sys
import tarfile
from datetime import datetime, timedelta, timezone
from environs import Env
from glob import glob
from os.path import basename, dirname, getmtime, getsize, exists, join
from pathlib import Path
from time import sleep

import pandas as pd
import sqlalchemy as sa
import xdg_base_dirs as xdg
from sqlalchemy import create_engine
# from tabulate import tabulate
# from xdg import XDG_DATA_HOME, XDG_CONFIG_HOME

appname = "MusicHistory"
config_dir = xdg.xdg_config_home()
if not config_dir:
	config_dir = os.path.expanduser("~/.config")
if appname:
	config_dir = os.path.join(config_dir, appname)
try:
	sys.path.insert(0, config_dir)
except ModuleNotFoundError:
	raise ModuleNotFoundError("config.py")
from config import Config  # noqa

__module__ = Path(__file__).resolve().stem


def main():
	process_date = (_run_dt - timedelta(days=1)).date()
	date_id = process_date.strftime("%Y%m%d")
	log_dir = _data_dir / date_id
	print(f"Log Dir: {log_dir}")
	merged_filename = log_dir / f"merged_{date_id}-test.log.xz"
	merge_logs(log_dir, merged_filename)

	playinfo_filename = log_dir / f"playinfo_{date_id}-test.csv.xz"

	df = extract_playinfo(process_date, merged_filename, playinfo_filename)
	print(df)

	# ToDo: Add table dt_playhistory3
	rows = df.to_sql("dt_playhistory3", con=engine, schema=schema,
					 if_exists="append", index=False)
	print(f"Added {rows} rows to database")

	return


def main_old():
	# Under normal conditions, start_date should be at least two days ago
	start_date = fetch_last_playdatetime_old().date()
	# start_date = datetime(2022, 12, 5).date()
	# Since we're only processing complete days, end_date is yesterday
	end_date = (_run_dt - timedelta(days=1)).date()
	day_delta = timedelta(days=1)
	os.chdir(_data_dir)
	if start_date < end_date:
		for i in range(1, (end_date - start_date).days + 1):
			process_date = start_date + i * day_delta
			date_id = process_date.strftime("%Y%m%d")
			logdir = Path(date_id)
			if not logdir.exists():
				continue
			##### 2025-02-21 PAL - Remove empty log files - BEGIN
			for entry in [x for x in os.scandir(logdir) if x.is_file()]:
				if getsize(entry.path) == 0:
					os.remove(entry.path)
			##### 2025-02-21 PAL - Remove empty log files - END

			##### 2022-12-06 PAL - Merge Log Files - BEGIN
			merged_filename = logdir / f"merged_{date_id}.log.xz"
			playinfo_filename = logdir / f"playinfo_{date_id}.csv.xz"
			logfiles = [
				f for f in os.scandir(logdir) if f.is_file()
					and f.name.startswith("smplayer")
					and f.name.endswith(".log")
			]
			# summaryfilename = logdir / f"summary_{date_id}.csv.xz"
			# tarfilename = logdir / f"smplayer_logs_{date_id}.txz"

			if playinfo_filename.exists() and not logfiles:
				continue
			elif logfiles:
				logfiles = sorted(logfiles, key=getmtime)
				last_mtime = getmtime(logfiles[-1])
				if not merged_filename.exists():
					# Create merged file from the first uncompressed log file
					with open(logfiles[0], "rb") as fin, lzma.open(
						merged_filename, "wb"
					) as fout:
						# Reads the file by chunks to avoid exhausting memory
						shutil.copyfileobj(fin, fout)
				total_count = 0
				for i in range(0, len(logfiles) - 1):
					j = i + 1
					count = find_new_lines_old(
						logfiles[i].path, logfiles[j].path, merged_filename
					)
					total_count += count
					do_nothing()
				# Set modification time to that of the most recent log file
				os.utime(merged_filename, times=(last_mtime, last_mtime))
			# ToDo: Replace summaryfilename and extract_loginfo()
			##### 2022-12-06 PAL - Merge Log Files - END

			# summaryfilename = logdir / f"summary_{date_id}.csv.xz"
			# tarfilename = logdir / f"smplayer_logs_{date_id}.txz"

			if not playinfo_filename.exists():
				# Only process log files that don't have a matching playlist file
				print(f"Processing: {process_date} . . .")
				# df = extract_loginfo(logdir, summaryfilename, tarfilename)
				df = extract_playinfo(process_date, merged_filename, playinfo_filename)
			else:
				print(f"Loading: '{playinfo_filename}'")
				df = pd.read_csv(playinfo_filename, sep="\t", parse_dates=["playdatetime"])
			# Fetch last row from database
			columns = [
				"epochtime",
				"playdate",
				"playdatetime",
				"filename",
				"play_secs",
				"play_time",
			]
			cols = ", ".join(columns)
			sql = f"SELECT {cols} FROM {tablename} ORDER BY epochtime DESC LIMIT 1;"
			last_df = pd.read_sql_query(sql, con=engine)

			# Merge last row from database
			df = pd.concat([last_df, df], ignore_index=True)
			df.drop_duplicates(subset=["epochtime"], inplace=True)

			# Calculate durations / play times
			df["play_secs"] = df.shift(-1)["epochtime"] - df["epochtime"]
			df["play_time"] = df.shift(-1)["playdatetime"] - df["playdatetime"]
			# df["play_time"].fillna(timedelta(0), inplace=True)
			# df["play_time"] = df["play_time"].astype(str)

			# Drop last row (was added, above)
			df.drop(0, inplace=True)

			# 2025-Mar-18 PAL - ToDo: Improve this logic to use current log file
			# Calculate play time for last row, using mtime of playinfo file
			# summary_mtime = summaryfilename.stat().st_mtime
			# summary_dt = datetime.fromtimestamp(summary_mtime).astimezone()
			playinfo_mtime = getmtime(playinfo_filename)
			playinfo_dt = datetime.fromtimestamp(playinfo_mtime).astimezone()

			""" 2024-Oct-15 PAL - FutureWarning for Pandas 3.0
			OLD:

			df["col"][row_indexer] = value

			NEW:

			df.loc[row_indexer, "col"] = values
			"""
			""" Old:
			# Prevent 'SettingWithCopyWarning' message from appearing
			pd.options.mode.chained_assignment = None
			df["play_secs"].iloc[-1] = summary_mtime - df["epochtime"].iloc[-1]
			df["play_time"].iloc[-1] = summary_dt - df["playdatetime"].iloc[-1]
			df["play_time"] = df["play_time"].astype(str)
			"""
			df.loc[df.index[-1], "play_secs"] = playinfo_mtime - df["epochtime"].iloc[-1]
			df.loc[df.index[-1], "play_time"] = playinfo_dt - df["playdatetime"].iloc[-1]
			df["play_time"] = df["play_time"].astype(str)

			# See if epochtimes already exist
			"""epochtimes_list = list(df["epochtime"].values)
			where_clause = "epochtime IN (%s)" % ",".join([str(x) for x in epochtimes_list])
			sql = f"SELECT epochtime FROM {schema}.{tablename} WHERE {where_clause};"
			existing_df = pd.read_sql_query(sql, con=engine)
			if not existing_df["epochtime"].empty:
				df = df.loc[df["epochtime"] != existing_df["epochtime"]]"""
			rows = df.to_sql(tablename, con=engine, schema=schema,
							 if_exists="append", index=False)
			print(f"Added {rows} rows to database")
			do_nothing()
	return


def init():
	print(f"Run Start: {__module__} v{__version__} {_run_dt}")
	return


def eoj():
	stop_dt = datetime.now().astimezone().replace(microsecond=0)
	duration = stop_dt.replace(microsecond=0) - _run_dt.replace(microsecond=0)
	print("Run Stop : %s  Duration: %s" % (stop_dt, duration))
	return


def do_nothing():
	pass


# ToDo: Re-engineer dt_playhistory to include stop time and exclude calculated columns like play_secs and play_time
def extract_playinfo(play_date: datetime.date, merged_filename: str | Path, playinfofilename: str | Path) -> pd.DataFrame:
	# print(f"Play Date:  . . . . . . {play_date}")
	# print(f"Log Filename  . . . . . {merged_filename}")
	# print(f"PlayInfo Filename . . . {playinfofilename}")
	#
	# Extract media & time information from merged log file
	start_regex = r"Core::startMplayer: file:"
	stop_regex = r"Core::stopMplayer$"
	play_regex = r"Playlist::playItem:"
	patterns = "|".join([start_regex, stop_regex, play_regex])

	if str(merged_filename).endswith(".xz"):
		open = lzma.open
	else:
		open = builtins.open
	with open(merged_filename, "rt") as logfile:
		lines = [x.rstrip() for x in logfile.readlines()]
	# print(f"Line Count: {len(lines)}  {merged_filename}")
	# info_lines = [x for x in lines if re.search(start_regex, x)]
	info_lines = [x for x in lines if re.search(patterns, x)]
	# print(f"Info Lines: {len(info_lines)}  {merged_filename}")

	csv_rows = []
	seen_epochs = []
	yr = play_date.year
	mo = play_date.month
	d = play_date.day

	last_play_dt = None
	# last_stop_dt = None
	last_filename = None
	for il in info_lines:
		parts = il.split()
		timepart = parts[0].lstrip("[").rstrip("]")
		h, m, s, ms = [int(x) for x in timepart.split(":")]
		dt = datetime(yr, mo, d, h, m, s, ms).astimezone()
		filename = " ".join(parts[3:-2]).strip('"')
		if filename:
			play_dt = dt
			stop_dt = None
			last_filename = filename
			last_play_dt = play_dt
		else:
			filename = last_filename
			stop_dt = dt
			last_stop_dt = stop_dt
			play_dt = last_play_dt
		if filename and stop_dt:
			epochtime = play_dt.timestamp()
			play_time = stop_dt - play_dt
			play_secs = play_time.total_seconds()
			if play_secs > 1800:
				do_nothing()
			if epochtime not in seen_epochs:
				row = (epochtime, play_date, play_dt.isoformat(" "), stop_dt.isoformat(" "), filename, play_secs, play_time)
				csv_rows.append(row)
				seen_epochs.append(epochtime)
			else:
				do_nothing()
	if csv_rows:
		if playinfofilename.exists() and Config.OVERWRITE_PLAYINFO_CSV:
			os.remove(playinfofilename)
		if not playinfofilename.exists():
			playinfofilename.parent.mkdir(exist_ok=True)
			columns = "epochtime playdate playdatetime stopdatetime filename play_secs play_time".split()
			df = pd.DataFrame(csv_rows, columns=columns)
			try:
				df["playdatetime"] = pd.to_datetime(df["playdatetime"], format='ISO8601')
				df["stopdatetime"] = pd.to_datetime(df["stopdatetime"], format='ISO8601')
				df["play_time"] = pd.to_timedelta(df["play_time"])
			except Exception as e:
				print(e, file=sys.stderr)
				do_nothing()
			df["playdate"] = pd.to_datetime(df["playdate"]).dt.date
			if Config.SAVE_SUMMARIES:
				# Save to tab-delimited file
				df.to_csv(playinfofilename, sep="\t", header=columns, index=False)
				mtime = getmtime(merged_filename)
				os.utime(playinfofilename, times=(mtime, mtime))
				os.utime(playinfofilename.parent, times=(mtime, mtime))
	return df


def fetch_last_playdatetime_old() -> datetime:
	sql = f"SELECT max(playdatetime) FROM {tablename};"
	with engine.connect() as conn:
		# rows = dict(conn.execute(sa.text(sql)).fetchone())["last_playdatetime"]
		playdatetime = conn.execute(sa.text(sql)).fetchone()[0]
	return playdatetime


def find_new_lines_old(
	older_filename: str | Path, newer_filename: str | Path, merged_filename: str | Path
) -> int:
	new_lines = []
	lines_to_compare = 100
	# Get the correct open() function
	if older_filename.endswith(".xz"):
		open1 = lzma.open
	else:
		open1 = builtins.open
	if newer_filename.endswith(".xz"):
		open2 = lzma.open
	else:
		open2 = builtins.open

	with open1(older_filename, "rt") as f1, open2(newer_filename, "rt") as f2:
		lines1 = f1.readlines()
		if not lines1:
			do_nothing()
		lines2 = f2.readlines()
		if not lines2:
			do_nothing()
		line_count1 = len(lines1)
		line_count2 = len(lines2)
		if line_count2 > line_count1:
			if lines1[-1] == lines2[line_count1 - 1]:
				# Top part of files match
				new_lines = lines2[line_count1:]
				do_nothing()
			else:
				# New file has all-new lines (should rarely happen, if ever)
				new_lines = lines2
				do_nothing()
		elif line_count1 == line_count2:
			# Same file? (Shouldn't happen)
			do_nothing()
		else:
			# Top part of files are different - SMPlayer was probably restarted
			new_lines = lines2
			do_nothing()
		if new_lines:
			if str(merged_filename).endswith(".xz"):
				open3 = lzma.open
			else:
				open3 = builtins.open
			with open3(merged_filename, "at") as fout:
				fout.writelines(new_lines)
		do_nothing()
	return len(new_lines)


def get_log_entries(filename: str | Path) -> list[str]:
	entries = []
	# Sample timestamp: [03:21:17:080]
	log_regex = r"^\[[012][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]{3}\]"
	if str(filename).endswith(".xz"):
		open = lzma.open
	else:
		open = builtins.open
	with open(filename, "rt") as logfile:
		lines = [x.rstrip() for x in logfile.readlines()]
	entry_lines = []
	for line in lines:
		if re.search(log_regex, line):
			if entry_lines:
				entries.append("\n".join(entry_lines))
			entry_lines = [line,]
		else:
			entry_lines.append(line)
	return entries


def merge_logs(log_dir: str | Path, merged_filename: str | Path):
	logfiles = sorted(glob(join(log_dir, "smplayer*log")))
	last_mtime = getmtime(logfiles[-1])
	if exists(merged_filename):
		merged_entries = get_log_entries(merged_filename)
	else:
		merged_entries = []
	found_new_entries = False
	for logfile in logfiles:
		new_entries = [
			x.rstrip() for x in get_log_entries(logfile) \
				if x not in merged_entries
		]
		if new_entries:
			merged_entries.extend(new_entries)
			found_new_entries = True
			do_nothing()
	if found_new_entries:
		if str(merged_filename).endswith(".xz"):
			open = lzma.open
		else:
			open = builtins.open
		with open(merged_filename, "wt") as fp:
			fp.writelines([f"{x}\n" for x in merged_entries])
	# Set modification time to that of the most recent log file
	os.utime(merged_filename, times=(last_mtime, last_mtime))
	return


def merge_logs_old(log_dir: str | Path, merged_filename: str | Path):
	logfiles = sorted(glob(join(log_dir, "smplayer*log")))
	last_mtime = getmtime(logfiles[-1])
	merged_lines = []
	if not exists(merged_filename):
		# Create merged file from the first uncompressed log file
		with (open(logfiles[0], "rb") as fin,
			  lzma.open(merged_filename, "wb") as fout):
			# Reads the file by chunks to avoid exhausting memory
			merged_lines
			shutil.copyfileobj(fin, fout)
		logfiles.pop(0)
	total_line_count = 0
	for i, logfile in enumerate(logfiles):
		# print(logfile)
		j = i + 1
		if j == len(logfiles):
			break
		line_count = find_new_lines_old(
			logfiles[i], logfiles[j], merged_filename
		)
		total_line_count += line_count
		do_nothing()
	# Set modification time to that of the most recent log file
	os.utime(merged_filename, times=(last_mtime, last_mtime))
	return


if __name__ == "__main__":
	_run_dt = datetime.now().astimezone().replace(microsecond=0)
	_run_utc = _run_dt.astimezone(timezone.utc).replace(tzinfo=None)
	_fdate = _run_dt.strftime("%Y-%m-%d")
	_fdatetime = _run_dt.strftime("%Y%m%d_%H%M%S")

	# Configure Directories
	_data_dir = xdg.xdg_data_home() / appname

	# Configure Database
	engine = create_engine(Config.DATABASE_URL)
	schema = Config.DB_SCHEMA
	tablename = Config.TABLE_NAME

	@sa.event.listens_for(engine, "connect", insert=True)
	def set_search_path(dbapi_connection, connection_record):
		"""
		Set schema search path in database
		"""
		sql = f"SET SESSION search_path TO {schema},public;"
		existing_autocommit = dbapi_connection.autocommit
		dbapi_connection.autocommit = True
		cursor = dbapi_connection.cursor()
		cursor.execute(sql)
		cursor.close()
		dbapi_connection.autocommit = existing_autocommit

	init()
	main()
	eoj()
